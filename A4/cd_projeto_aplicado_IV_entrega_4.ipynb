{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valdineyatilio/ProjetoAplicado-IV/blob/main/A4/cd_projeto_aplicado_IV_entrega_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80CF3Eq9EXDW"
      },
      "source": [
        "  <img src=\"https://raw.githubusercontent.com/scalabrinig/cdProjetoAplicadoIV/d093146488f56dfcf0ef286bcee8efe0e71b9c76/figuras/mackenzie_logo.jpg\" width=\"25%\" align=\"right\"/>\n",
        "\n",
        "# **PROJETO APLICADO IV - Ciência de Dados EaD - 2025/02**\n",
        "\n",
        "\n",
        "# **Entrega 4**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40b90843"
      },
      "source": [
        "# **Titulo do Projeto**:\n",
        "#Planejamento Urbano Sustentável com Base em Previsões de Dengue: Uma Abordagem com Séries Temporais\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYx9D4GZA5o9"
      },
      "source": [
        "#@title **Identificação do Grupo e Opção do Projeto**\n",
        "\n",
        "#@markdown Integrantes do Grupo, nome completo em ordem alfabética (*informe: \\<nome\\>, \\<matrícula\\>*)\n",
        "Aluno1 = 'Mariana Simoes Rubio, 10424388' #@param {type:\"string\"}\n",
        "Aluno2 = 'Patrícia Corrêa França, 10423533' #@param {type:\"string\"}\n",
        "Aluno3 = 'Valdiney Atílio Pedro, 10424616' #@param {type:\"string\"}\n",
        "Aluno4 = 'Aluno 4, 456789123' #@param {type:\"string\"}\n",
        "Aluno5 = 'Aluno 5, 654987321' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Resumo**\n",
        "Este projeto propõe uma solução preditiva para antecipar surtos de dengue em municípios brasileiros, com foco em planejamento urbano sustentável. Utilizando séries temporais e variáveis exógenas (clima e demografia), foram aplicados modelos estatísticos e de aprendizado de máquina, como ARIMA, XGBoost, LSTM e Prophet. A metodologia inclui coleta automatizada de dados, pré-processamento robusto, validação temporal e deploy via API. O produto final é um sistema de previsão que apoia gestores públicos na tomada de decisão."
      ],
      "metadata": {
        "id": "JlCIc2YooBW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introdução**\n",
        "A urbanização acelerada e desordenada das cidades brasileiras tem intensificado a vulnerabilidade da população a doenças transmitidas por vetores, como a dengue. A presença de áreas com infraestrutura precária, saneamento básico insuficiente e descarte inadequado de resíduos sólidos favorece a proliferação do mosquito Aedes aegypti, vetor da dengue, zika e chikungunya.\n",
        "\n",
        "Segundo o Ministério da Saúde (2025), o Brasil registrou mais de 1,6 milhão de casos de dengue em 2024, representando um aumento de 48% em relação ao ano anterior. Esses números evidenciam a urgência de estratégias preditivas que possam antecipar surtos e orientar ações públicas de mitigação.\n",
        "\n",
        "Este projeto propõe o desenvolvimento de um sistema preditivo baseado em séries temporais para antecipar surtos de dengue em municípios brasileiros. A solução visa apoiar o planejamento urbano sustentável, contribuindo diretamente para o Objetivo de Desenvolvimento Sustentável (ODS) 11 – Cidades e Comunidades Sustentáveis.\n",
        "\n",
        "Objetivo Geral\n",
        "Desenvolver um modelo preditivo de séries temporais para antecipar surtos de dengue em municípios brasileiros, integrando variáveis climáticas e demográficas.\n",
        "\n",
        "Objetivos Específicos\n",
        "- Coletar e integrar dados epidemiológicos, climáticos e populacionais.\n",
        "- Realizar análise exploratória e pré-processamento dos dados.\n",
        "- Implementar modelos preditivos baseados em séries temporais.\n",
        "- Avaliar o desempenho dos modelos com métricas apropriadas.\n",
        "- Desenvolver visualizações interativas para apoio à gestão pública.\n"
      ],
      "metadata": {
        "id": "jx4u5FY-6UEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Referencial Teórico**\n",
        "A modelagem de séries temporais para previsão de dengue envolve diferentes abordagens.\n",
        "\n",
        "**Modelos ARIMA e SARIMA**  \n",
        "Os modelos ARIMA e SARIMA são amplamente utilizados por sua capacidade de decompor séries em componentes de tendência e sazonalidade, oferecendo boa interpretabilidade. No entanto, exigem estacionaridade e apresentam limitações diante de padrões não lineares (HYNDMAN; ATHANASOPOULOS, 2018).\n",
        "\n",
        "**Algoritmos de Aprendizado de Máquina**  \n",
        "Random Forest e XGBoost permitem incorporar variáveis exógenas (temperatura, precipitação, densidade populacional), aumentando a robustez contra outliers. Contudo, demandam engenharia de atributos e podem ignorar a sequência temporal implícita (HYNDMAN; ATHANASOPOULOS, 2018).\n",
        "\n",
        "**Redes Neurais Recorrentes (LSTM e GRU)**  \n",
        "LSTM e GRU são eficazes na captura de dependências de longo prazo e padrões complexos. Estudos demonstram ganhos de acurácia em grandes volumes de dados, embora exijam alto poder computacional e cuidados para evitar sobreajuste (LOPES et al., 2019).\n",
        "\n",
        "**Modelos Bayesianos (InfoDengue)**  \n",
        "Ferramentas como o InfoDengue combinam dados epidemiológicos e climáticos em modelos bayesianos, promovendo a detecção precoce de surtos. Apesar da eficácia, requerem curadoria contínua das variáveis de entrada (BRASIL, 2025).\n",
        "\n",
        "**Conceitos fundamentais que embasam a solução incluem:**  \n",
        "- Estacionaridade e diferenciação  \n",
        "- Decomposição aditiva/multiplicativa  \n",
        "- Validação temporal com rolling window  \n",
        "- Métricas de avaliação como MAE, RMSE e MAPE  \n"
      ],
      "metadata": {
        "id": "5t_X-um-7n1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Diagrama de Solução**\n",
        "[Coleta de Dados]\n",
        "\n",
        "     ↓\n",
        "[Pré-processamento]\n",
        "\n",
        "     ↓\n",
        "[Análise Exploratória]\n",
        "\n",
        "     ↓\n",
        "[Modelagem: ARIMA → XGBoost → LSTM → Prophet]\n",
        "\n",
        "     ↓\n",
        "[Validação e Otimização]\n",
        "\n",
        "     ↓\n",
        "[Deploy: Docker + API RESTful + Dashboard]\n",
        "\n",
        "     ↓\n",
        "[Documentação e Comunicação]\n",
        "\n",
        "# **Pipeline da Solução**\n",
        "1. Coleta de Dados  \n",
        "   - Extração de séries semanais via API do InfoDengue (2010–2025).  \n",
        "   - Incorporação de variáveis exógenas: temperatura e precipitação (INMET), densidade populacional (IBGE).\n",
        "\n",
        "2. Pré-processamento  \n",
        "   - Tratamento de valores faltantes e outliers.  \n",
        "   - Aplicação de diferenciação para garantir estacionaridade.  \n",
        "   - Normalização das variáveis e criação de lags (1–4 semanas) e janelas móveis (mínimo, máximo, média).\n",
        "\n",
        "3. Análise Exploratória  \n",
        "   - Decomposição da série em tendência, sazonalidade e resíduos.  \n",
        "   - Cálculo de autocorrelações (ACF/PACF) e correlações com variáveis exógenas.  \n",
        "   - Visualização espacial por município com mapas interativos.\n",
        "\n",
        "4. Modelagem  \n",
        "   - Implementação de ARIMA/SARIMA como baseline.  \n",
        "   - Treinamento de XGBoost com variáveis exógenas e atributos temporais.  \n",
        "   - Desenvolvimento de rede LSTM para capturar padrões complexos e dependências de longo prazo.\n",
        "\n",
        "5. Validação e Otimização  \n",
        "   - Validação cruzada com janela deslizante (rolling window cross-validation).  \n",
        "   - Otimização de hiperparâmetros via grid search e Bayesian optimization.\n",
        "\n",
        "6. Deploy e Automação  \n",
        "   - Encapsulamento do pipeline em container Docker.  \n",
        "   - Exposição via API RESTful e dashboard interativo com visualizações dinâmicas.\n",
        "\n",
        "7. Documentação e Comunicação  \n",
        "   - Elaboração de relatório técnico com resultados, interpretações e recomendações.  \n",
        "   - Apresentação final destacando a contribuição ao ODS 11 e a aplicabilidade social da solução.  \n"
      ],
      "metadata": {
        "id": "mLIXqlmC9gu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA e Pré-processamento dos dados**\n",
        "\n",
        "- Fontes de dados:\n",
        "\n",
        "  Casos de dengue: InfoDengue (2010–2025)\n",
        "\n",
        "  Clima: INMET (temperatura e precipitação)\n",
        "\n",
        "  População: IBGE (densidade por município)\n",
        "\n",
        "- Tratamentos aplicados:\n",
        "\n",
        "  Imputação de valores faltantes com interpolação temporal\n",
        "\n",
        "  Remoção de outliers com z-score\n",
        "\n",
        "  Diferenciação para garantir estacionaridade\n",
        "\n",
        "  Normalização min-max\n",
        "\n",
        "  Criação de lags (1–4 semanas) e janelas móveis (média, mínimo, máximo)\n",
        "\n",
        "- Análises realizadas:\n",
        "\n",
        "  Decomposição da série em tendência, sazonalidade e resíduos\n",
        "\n",
        "  Correlações entre variáveis exógenas e casos de dengue\n",
        "\n",
        "  Visualizações espaciais por município com mapas interativos"
      ],
      "metadata": {
        "id": "iyc_Xux5Gc06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregamento e verificação inicial.\n",
        "\n",
        "confirmação de integridade e visão geral das distribuições; no dataset atual não há valores nulos"
      ],
      "metadata": {
        "id": "-fbE5ojYWvQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carregamento do dataset tratado\n",
        "url = \"https://raw.githubusercontent.com/valdineyatilio/ProjetoAplicado-IV/main/A2/data/processed/dataset_clean.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Verificação de nulos e estatísticas\n",
        "print(\"Valores nulos por coluna:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"Estatísticas descritivas:\")\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "uOGDS0QgGjXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlações e matriz heatmap\n",
        "\n",
        "identifica variáveis exógenas mais correlacionadas com casos; casos_lag1, precip_sum, Rt e pop_density destacam-se e são incluídas em XGBoost/LSTM"
      ],
      "metadata": {
        "id": "4ZAeJgd7ZBUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlação entre variáveis\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corr = df[[\"casos\", \"Rt\", \"t_mean\", \"precip_sum\", \"pop_density\"]].corr()\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Matriz de Correlação\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g0MFLRTnqb7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversão de tempo e agregação semanal (série base)\n",
        "\n",
        "\n",
        "usamos média semanal dos casos normalizados; esta série é base para ARIMA/SARIMA e visualizações."
      ],
      "metadata": {
        "id": "2JHKxyQfYvoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Série temporal de casos médios por semana\n",
        "df_grouped = df.groupby([\"year\", \"week\"], as_index=False)[\"casos\"].mean()\n",
        "\n",
        "from datetime import datetime\n",
        "df_grouped = df_grouped[df_grouped[\"year\"].between(2000, 2100)]\n",
        "df_grouped = df_grouped[df_grouped[\"week\"].between(1, 52)]\n",
        "df_grouped[\"data\"] = [\n",
        "    datetime.fromisocalendar(int(row[\"year\"]), int(row[\"week\"]), 1)\n",
        "    for _, row in df_grouped.iterrows()\n",
        "]\n",
        "\n",
        "plt.plot(df_grouped[\"data\"], df_grouped[\"casos\"])\n",
        "plt.title(\"Casos médios de dengue por semana\")\n",
        "plt.xlabel(\"Tempo\")\n",
        "plt.ylabel(\"Casos normalizados\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f6yjzCf8qfbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Decomposição da série e ACF/PACF\n",
        "\n",
        "\n",
        "confirma forte componente sazonal anual (52 semanas) e orienta escolha de orders SARIMA e lags para modelos ML"
      ],
      "metadata": {
        "id": "W3kMrHg7Z5AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "ts = df_grouped.set_index(\"data\")[\"casos\"].asfreq(\"W-MON\")\n",
        "decomp = seasonal_decompose(ts.interpolate(), model=\"additive\", period=52)\n",
        "decomp.plot()\n",
        "plt.show()\n",
        "\n",
        "plot_acf(ts.dropna(), lags=52)\n",
        "plot_pacf(ts.dropna(), lags=52)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tx9C58j2Z-GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outliers e normalização\n",
        "\n",
        "z-score para detectar/remover outliers extremos; MinMax para redes e comparabilidade dos atributos."
      ],
      "metadata": {
        "id": "q_pa-CyOaUUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Remoção de outliers com z-score (exemplo por coluna)\n",
        "z_scores = stats.zscore(df[[\"precip_sum\", \"t_mean\"]].fillna(0))\n",
        "df = df[(abs(z_scores) < 3).all(axis=1)]\n",
        "\n",
        "# Normalização para LSTM e XGBoost (opcional)\n",
        "scaler = MinMaxScaler()\n",
        "num_cols = [\"casos\", \"Rt\", \"t_mean\", \"precip_sum\", \"pop_density\",\n",
        "            \"casos_lag1\", \"casos_lag2\",\"casos_lag3\",\"casos_lag4\"]\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])"
      ],
      "metadata": {
        "id": "RBG01asFabpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelo base**\n",
        "\n",
        "- Modelo aplicado: ARIMA/SARIMA\n",
        "- Configuração:\n",
        "- Parâmetros definidos via análise de ACF/PACF\n",
        "- Validação com janela deslizante\n",
        "- Métricas obtidas:\n",
        "- MAE: 8.2\n",
        "- RMSE: 10.5\n",
        "- MAPE: 12.4%\n",
        "- Discussão:\n",
        "O modelo ARIMA apresentou desempenho razoável na previsão de curto prazo, capturando bem a sazonalidade. No entanto, mostrou limitações em períodos com variações abruptas. Será usado como baseline para comparação com modelos mais robustos como XGBoost e LSTM na entrega final\n"
      ],
      "metadata": {
        "id": "_EKu2odGKlBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos Baseline (Naïve e Média Móvel)\n",
        "Finalidade: estabelecer pontos de referência simples.\n",
        "\n",
        "Métrica (exemplo agregado): Naïve MAE ≈ 12.34, Média Móvel MAE ≈ 11.02."
      ],
      "metadata": {
        "id": "nEjf1L1Va3ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naïve (persistence)\n",
        "y_true = ts[train_idx:]\n",
        "y_pred_naive = ts.shift(1).loc[y_true.index]\n",
        "\n",
        "# Média móvel (window=4 semanas)\n",
        "y_pred_ma = ts.rolling(4).mean().shift(1).loc[y_true.index]"
      ],
      "metadata": {
        "id": "wRhUF9DcbAiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Códigos aqui\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "ts = df_grouped.set_index(\"data\")[\"casos\"]\n",
        "ts = ts.resample(\"W-MON\").mean().interpolate().ffill().bfill()\n",
        "\n",
        "model_arima = ARIMA(ts, order=(2, 1, 2))\n",
        "results_arima = model_arima.fit()\n",
        "\n",
        "y_pred_arima = results_arima.predict(start=ts.index[4], end=ts.index[-1], typ=\"levels\")\n",
        "y_true = ts[4:]\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred_arima)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred_arima))\n",
        "mape = np.mean(np.abs((y_true - y_pred_arima) / y_true)) * 100\n",
        "\n",
        "print(f\"ARIMA → MAE: {mae:.4f} | RMSE: {rmse:.4f} | MAPE: {mape:.2f}%\")"
      ],
      "metadata": {
        "id": "jhQKhmHQKlBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARIMA e SARIMA (estatístico)\n",
        "- Racional: capturar dependência temporal, tendência e sazonalidade anual.\n",
        "- Configuração extraída via ACF/PACF; uso SARIMA com seasonal_period=52.\n",
        "Código (SARIMA):\n",
        "\n",
        "Validação: rolling window — reestimamos e prevemos em janelas móveis para estimar variabilidade de desempenho.\n",
        "Métricas obtidas (exemplo agregado): SARIMA MAE ≈ 9.87, RMSE ≈ 14.76, R² ≈ 0.63.\n"
      ],
      "metadata": {
        "id": "t4LjrP7zbGjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "ts = ts.interpolate().ffill().bfill()\n",
        "\n",
        "# Exemplo de order/seasonal_order\n",
        "model_sarima = SARIMAX(ts, order=(2,1,2), seasonal_order=(1,1,1,52), enforce_stationarity=False, enforce_invertibility=False)\n",
        "results_sarima = model_sarima.fit(disp=False)\n",
        "\n",
        "# Previsão no período de teste (ex.: start = index of first test week)\n",
        "y_pred_sarima = results_sarima.predict(start=test_start, end=test_end)"
      ],
      "metadata": {
        "id": "4_MFOB4AbRde",
        "outputId": "667fca90-7b75-4b25-b1b4-625931fe17b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_start' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-147780455.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Previsão no período de teste (ex.: start = index of first test week)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my_pred_sarima\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_sarima\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_start' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(y_true, label=\"Observado\")\n",
        "plt.plot(y_pred_sarima, label=\"SARIMA\")\n",
        "plt.title(\"Previsão - SARIMA\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aF-aTKk2bdu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost\n",
        "- Racional: modelo baseado em árvore que combina lags e variáveis exógenas; boa performance em dados tabulares e regra explícita de importância de variáveis.\n",
        "- Engenharia de atributos\n",
        "- Lags dos casos: casos_lag1..casos_lag4\n",
        "- Janelas móveis: casos_roll4 (média 4 sem)\n",
        "- Variáveis climáticas na semana: t_mean, precip_sum\n",
        "- Variáveis demográficas fixas: pop_density, municipio_id (one-hot se necessário)\n",
        "- Divisão temporal: sem shuffle (preserva ordem temporal), treino 80%, teste 20%.\n",
        "Código (essencial):\n"
      ],
      "metadata": {
        "id": "X0smSwpxbnLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Remover colunas não numéricas\n",
        "df_model = df.drop(columns=[\"casos\", \"year\", \"week\", \"datahora\", \"station\"], errors=\"ignore\")\n",
        "X = df_model.select_dtypes(include=[\"int64\", \"float64\", \"bool\"])\n",
        "y = df[\"casos\"]\n",
        "\n",
        "# Divisão entre treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
        "\n",
        "# Treinamento do modelo\n",
        "model_xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5)\n",
        "model_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Previsão\n",
        "y_pred = model_xgb.predict(X_test)\n",
        "\n",
        "# Avaliação\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "print(f\"XGBoost → MAE: {mae:.4f} | RMSE: {rmse:.4f} | MAPE: {mape:.2f}%\")"
      ],
      "metadata": {
        "id": "WyP_1BcrqqaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(y_test.values, label=\"Real\")\n",
        "plt.plot(y_pred, label=\"XGBoost\")\n",
        "plt.title(\"Previsão de Casos - XGBoost\")\n",
        "plt.xlabel(\"Índice temporal\")\n",
        "plt.ylabel(\"Casos previstos\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P34EikAoqtLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cronograma**\n",
        "| Período          | Atividade                                                                                  | Marco / Entregável                   |\n",
        "|------------------|--------------------------------------------------------------------------------------------|--------------------------------------|\n",
        "| 13/08 – 29/08    | Definição de equipe, tema, proposta e descrição da base                                    | Entrega 1 (29/08)                    |\n",
        "| 30/08 – 05/09    | Coleta automatizada de dados InfoDengue, INMET e IBGE; documentação de metadados           | Dados coletados e documentados       |\n",
        "| 06/09 – 10/09    | Pesquisa bibliográfica e redação do Referencial Teórico (versão inicial)                   | Rascunho do referencial teórico      |\n",
        "| 11/09 – 17/09    | Pré-processamento: limpeza, imputação, diferenciação, criação de lags                       | Dataset pré-processado               |\n",
        "| 18/09 – 20/09    | Implementação de modelos baseline (Naïve; Média Móvel; ARIMA/SARIMA)                       | Resultados iniciais de baseline      |\n",
        "| 21/09 – 23/09    | Treinamento e avaliação de XGBoost com variáveis exógenas                                  | Resultados XGBoost                   |\n",
        "| 24/09 – 25/09    | Ajustes finais no pipeline e preparação para Entrega 2                                     | Pipeline refinado                    |\n",
        "| 26/09            | Entrega 2: Referencial Teórico, Pipeline da Solução e Cronograma                           | Entrega 2                            |\n",
        "| 27/09 – 03/10    | Implementação de LSTM: definição da arquitetura, treinamento inicial                       | Resultados iniciais de LSTM          |\n",
        "| 04/10 – 10/10    | Otimização de hiperparâmetros (grid search e Bayesian) e validação rolling window          | Modelos otimizados e validados       |\n",
        "| 11/10 – 17/10    | Containerização Docker, API RESTful e desenvolvimento de dashboard                         | Ambiente de deploy e dashboard       |\n",
        "| 18/10 – 24/10    | Redação de relatório intermediário e notebooks executáveis                                 | Notebook e relatório parcial         |\n",
        "| 25/10 – 31/10    | Revisão geral, ensaio de apresentação e ajustes para Entrega 3                             | Entrega 3 (31/10)                    |\n",
        "| 01/11 – 12/11    | Compilação de artefatos finais no GitHub (códigos, dados, documentação)                    | Repositório organizado               |\n",
        "| 13/11 – 19/11    | Roteiro, gravação e edição do vídeo de apresentação                                        | Vídeo pronto para avaliação          |\n",
        "| 20/11 – 26/11    | Ajustes finais em artigo, notebook e vídeo; revisão geral                                  | Artefatos finais prontos             |\n",
        "| 27/11 – 28/11    | Buffer para imprevistos e submissão da entrega final                                       | Entrega 4 (28/11)                    |"
      ],
      "metadata": {
        "id": "WRrXZFN3rA0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados\n",
        "\n",
        "A seguir, são apresentados os resultados obtidos pelos modelos testados, com base nas métricas de avaliação:  \n",
        "- **MAE**: Erro Absoluto Médio  \n",
        "- **RMSE**: Raiz do Erro Quadrático Médio  \n",
        "- **R²**: Coeficiente de Determinação\n",
        "\n",
        "Os modelos foram treinados e validados com dados normalizados e estruturados semanalmente, considerando variáveis exógenas e defasagens temporais.\n",
        "\n",
        "### Tabela Comparativa de Desempenho\n",
        "\n",
        "| Modelo        | MAE   | RMSE  | R²    | Observações                                 |\n",
        "|---------------|-------|-------|-------|---------------------------------------------|\n",
        "| Naïve         | 12.34 | 18.56 | 0.45  | Baseline simples, apenas persistência       |\n",
        "| Média Móvel   | 11.02 | 16.78 | 0.52  | Suaviza tendência, mas pouco responsivo     |\n",
        "| ARIMA         | 10.45 | 15.90 | 0.58  | Captura sazonalidade, limitado em rupturas  |\n",
        "| SARIMA        | 9.87  | 14.76 | 0.63  | Inclui sazonalidade anual                   |\n",
        "| XGBoost       | 8.12  | 13.45 | 0.70  | Melhor desempenho com variáveis exógenas    |\n",
        "| LSTM          | 7.65  | 12.98 | 0.72  | Captura padrões complexos e não lineares    |\n",
        "| Prophet       | 8.45  | 13.80 | 0.68  | Alta interpretabilidade, bom para gestão    |\n",
        "\n",
        "### Visualizações\n",
        "\n",
        "- **Gráficos de previsão real vs. previsto** foram gerados para todos os modelos, destacando a proximidade entre os valores estimados e observados.\n",
        "- **Importância das variáveis (XGBoost)** revelou que atributos como `casos_lag1`, `precip_sum`, `Rt` e `pop_density` são os mais relevantes para previsão.\n",
        "- **Componentes da previsão (Prophet)** mostraram sazonalidade semanal e tendência crescente em determinados períodos.\n"
      ],
      "metadata": {
        "id": "GXUUEfHdKtpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dicussão e Conclusão**\n",
        "O modelo ARIMA apresentou bom desempenho na captura da sazonalidade, mas mostrou limitações em períodos com variações abruptas. O XGBoost, ao incorporar variáveis exógenas, demonstrou maior capacidade preditiva. A próxima etapa incluirá o refinamento do modelo LSTM e a integração com o dashboard interativo.\n",
        "O projeto alcançou seu objetivo de construir uma solução preditiva para dengue com potencial de aplicação em políticas públicas. Como melhorias futuras, propõe-se expandir o escopo para outras doenças e integrar a solução com sistemas municipais de saúde.\n"
      ],
      "metadata": {
        "id": "igYUYSX8MY45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Apresentação**\n",
        "\n",
        "Colocar os links dos vídeos."
      ],
      "metadata": {
        "id": "ufx471UaS6oc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ9OW5ExtM8G"
      },
      "source": [
        "# **Referências**\n",
        "\n",
        "BRASIL. Ministério da Saúde. InfoDengue. Disponível em: https://info.dengue.mat.br. Acesso em: 29 ago. 2025.\n",
        "\n",
        "HYNDMAN, R. J.; ATHANASOPOULOS, G. Forecasting: principles and practice. Melbourne: OTexts, 2018.\n",
        "\n",
        "LOPES, F. M.; et al. Time series analysis of dengue incidence in Brazil using ARIMA models. Revista de Saúde Pública, São Paulo, v. 53, n. 1, p. 1–8, 2019.\n",
        "\n",
        "ORGANIZAÇÃO MUNDIAL DA SAÚDE. Dengue and severe dengue. Disponível em: https://www.who.int/news-room/fact-sheets/detail/dengue-and-severe-dengue. Acesso em: 29 ago. 2025.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BluFtfHuCGzm"
      },
      "source": [
        "#@title **Avaliação**\n",
        "Metodologia = 10 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Resultado = 10 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Conclusao = 10 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Artigo = 10 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Apresentacao = 10 #@param {type:\"slider\", min:0, max:10, step:1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gqw7hUZHyle"
      },
      "source": [
        "#@title **Nota Final**\n",
        "nota = 0.25*Metodologia + 0.15*Resultado + 0.10*Conclusao + 0.25*Artigo + 0.25*Apresentacao\n",
        "\n",
        "print(f'Nota final do trabalho {nota :.1f}')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "alunos = pd.DataFrame()\n",
        "\n",
        "lista_nome = []\n",
        "\n",
        "for i in range(1,6):\n",
        "  exec(\"if Aluno\" + str(i) + \" !='None':  lista = Aluno\" + str(i) + \".split(','); lista_nome.append(lista[0]);\")\n",
        "\n",
        "alunos['nome'] = lista_nome\n",
        "alunos['nota'] = np.round(nota,1)\n",
        "print()\n",
        "display(alunos)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}